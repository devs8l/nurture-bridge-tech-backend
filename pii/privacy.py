"""
PII/PHI Privacy Module
Automated detection and anonymization using Microsoft Presidio

HIPAA Safe Harbor compliant
GDPR Article 32 compliant

NOTE: Presidio engines use lazy loading to reduce memory footprint.
They are only initialized when PII detection/anonymization functions are first called.

Generated by inquantic-foundry
"""

from typing import List, Dict, Any, Optional

from app_logging.logger import get_logger

logger = get_logger(__name__)

# ============================================================================
# LAZY LOADING CACHE
# ============================================================================
# Engines are initialized only when first needed to reduce memory footprint

_analyzer = None
_anonymizer = None


def _get_analyzer():
    """Lazy initialization of Presidio AnalyzerEngine"""
    global _analyzer
    if _analyzer is None:
        from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern
        
        # Initialize analyzer (detects PII/PHI)
        _analyzer = AnalyzerEngine()
        
        # Medical Record Number (MRN) recognizer
        mrn_recognizer = PatternRecognizer(
            supported_entity="MEDICAL_RECORD_NUMBER",
            patterns=[Pattern("MRN", r"\bMRN[-:\s]?\d{6,10}\b", 0.8)],
        )
        
        # Add custom healthcare identifiers
        _analyzer.registry.add_recognizer(mrn_recognizer)
    
    return _analyzer


def _get_anonymizer():
    """Lazy initialization of Presidio AnonymizerEngine"""
    global _anonymizer
    if _anonymizer is None:
        from presidio_anonymizer import AnonymizerEngine
        
        # Initialize anonymizer (masks/encrypts PII/PHI)
        _anonymizer = AnonymizerEngine()
    
    return _anonymizer


# ============================================================================
# PII DETECTION
# ============================================================================

def detect_pii(
        text: str,
        language: str = "en",
        entities: Optional[List[str]] = None,
        score_threshold: float = 0.5
) -> List[Dict[str, Any]]:
    """
    Detect PII/PHI in text.

    Args:
        text: Text to analyze
        language: Language code (default: en)
        entities: Specific entities to detect (None = all)
        score_threshold: Minimum confidence score

    Returns:
        List of detected PII entities with locations and types

    Example:
        results = detect_pii("John's email is john@example.com")
        # [{"type": "EMAIL_ADDRESS", "start": 18, "end": 35, "score": 0.85}]
    """
    try:
        analyzer = _get_analyzer()
        results = analyzer.analyze(
            text=text,
            language=language,
            entities=entities,
            score_threshold=score_threshold
        )

        detected = [
            {
                "type": result.entity_type,
                "start": result.start,
                "end": result.end,
                "score": result.score,
                "text": text[result.start:result.end]
            }
            for result in results
        ]

        if detected:
            logger.info(
                "pii_detected",
                count=len(detected),
                types=[d["type"] for d in detected]
            )

        return detected

    except Exception as e:
        logger.error("pii_detection_failed", error=str(e))
        return []


# ============================================================================
# PII ANONYMIZATION
# ============================================================================

def anonymize_pii(
        text: str,
        anonymization_type: str = "mask",
        language: str = "en",
        entities: Optional[List[str]] = None
) -> str:
    """
    Anonymize PII/PHI in text.

    Args:
        text: Text to anonymize
        anonymization_type: Type of anonymization
            - "mask": Replace with asterisks (default)
            - "replace": Replace with placeholder
            - "hash": Replace with hash
            - "encrypt": Encrypt (requires encryption key)
        language: Language code
        entities: Specific entities to anonymize (None = all)

    Returns:
        Anonymized text

    Example:
        result = anonymize_pii("My SSN is 123-45-6789")
        # "My SSN is <SSN>"
    """
    try:
        # First detect PII
        analyzer = _get_analyzer()
        analysis_results = analyzer.analyze(
            text=text,
            language=language,
            entities=entities
        )

        if not analysis_results:
            return text

        # Configure anonymization operators
        from presidio_anonymizer.entities import OperatorConfig
        operators = {}

        if anonymization_type == "mask":
            # Mask with asterisks
            for result in analysis_results:
                operators[result.entity_type] = OperatorConfig("mask", {
                    "masking_char": "*",
                    "chars_to_mask": 100,
                    "from_end": False
                })

        elif anonymization_type == "replace":
            # Replace with placeholder
            for result in analysis_results:
                operators[result.entity_type] = OperatorConfig("replace", {
                    "new_value": f"<{result.entity_type}>"
                })

        elif anonymization_type == "hash":
            # Replace with hash
            for result in analysis_results:
                operators[result.entity_type] = OperatorConfig("hash", {
                    "hash_type": "sha256"
                })

        else:  # Default to replace
            for result in analysis_results:
                operators[result.entity_type] = OperatorConfig("replace", {
                    "new_value": f"<{result.entity_type}>"
                })

        # Anonymize
        anonymizer = _get_anonymizer()
        result = anonymizer.anonymize(
            text=text,
            analyzer_results=analysis_results,
            operators=operators
        )

        logger.info(
            "pii_anonymized",
            original_length=len(text),
            anonymized_length=len(result.text),
            entities_anonymized=len(analysis_results)
        )

        return result.text

    except Exception as e:
        logger.error("pii_anonymization_failed", error=str(e))
        return text


# ============================================================================
# SPECIFIC ENTITY ANONYMIZATION
# ============================================================================

def mask_email(email: str) -> str:
    """Mask email address (keep first char and domain)"""
    try:
        local, domain = email.split("@")
        if len(local) > 1:
            masked_local = local[0] + "*" * (len(local) - 1)
        else:
            masked_local = "*"
        return f"{masked_local}@{domain}"
    except:
        return "***@***.***"


def mask_phone(phone: str) -> str:
    """Mask phone number (keep last 4 digits)"""
    digits = ''.join(filter(str.isdigit, phone))
    if len(digits) >= 4:
        return "*" * (len(digits) - 4) + digits[-4:]
    return "***-***-****"


def mask_ssn(ssn: str) -> str:
    """Mask SSN (keep last 4 digits)"""
    digits = ''.join(filter(str.isdigit, ssn))
    if len(digits) == 9:
        return "***-**-" + digits[-4:]
    return "***-**-****"


def mask_credit_card(card: str) -> str:
    """Mask credit card (keep last 4 digits)"""
    digits = ''.join(filter(str.isdigit, card))
    if len(digits) >= 4:
        return "**** **** **** " + digits[-4:]
    return "**** **** **** ****"


# ============================================================================
# BATCH PROCESSING
# ============================================================================

def anonymize_dict(
        data: Dict[str, Any],
        sensitive_fields: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Anonymize sensitive fields in dictionary.

    Args:
        data: Dictionary to anonymize
        sensitive_fields: List of field names to anonymize (None = auto-detect)

    Returns:
        Anonymized dictionary
    """
    if sensitive_fields is None:
        # Auto-detect common sensitive fields
        sensitive_fields = [
            "email", "phone", "ssn", "social_security",
            "credit_card", "password", "address", "dob",
            "date_of_birth", "medical_record_number", "mrn"
        ]

    result = {}

    for key, value in data.items():
        if isinstance(value, dict):
            # Recursively anonymize nested dicts
            result[key] = anonymize_dict(value, sensitive_fields)

        elif isinstance(value, list):
            # Anonymize list items
            result[key] = [
                anonymize_dict(item, sensitive_fields) if isinstance(item, dict)
                else anonymize_pii(str(item)) if isinstance(item, str)
                else item
                for item in value
            ]

        elif isinstance(value, str) and any(field in key.lower() for field in sensitive_fields):
            # Anonymize string value
            result[key] = anonymize_pii(value, anonymization_type="replace")

        else:
            result[key] = value

    return result


# ============================================================================
# VALIDATION
# ============================================================================

def contains_pii(text: str, threshold: float = 0.5) -> bool:
    """
    Check if text contains PII.

    Args:
        text: Text to check
        threshold: Minimum confidence score

    Returns:
        True if PII detected, False otherwise
    """
    detected = detect_pii(text, score_threshold=threshold)
    return len(detected) > 0


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

"""
# In your code:

from pii.privacy import detect_pii, anonymize_pii, anonymize_dict, contains_pii

# Detect PII
text = "Contact John Doe at john.doe@example.com or 555-123-4567"
pii_found = detect_pii(text)
# [{"type": "EMAIL_ADDRESS", ...}, {"type": "PHONE_NUMBER", ...}]

# Anonymize text
anonymized = anonymize_pii(text, anonymization_type="replace")
# "Contact John Doe at <EMAIL_ADDRESS> or <PHONE_NUMBER>"

# Check if contains PII
if contains_pii(user_input):
    logger.warning("User input contains PII")

# Anonymize dictionary
user_data = {
    "name": "John Doe",
    "email": "john@example.com",
    "age": 30
}
safe_data = anonymize_dict(user_data)
# {"name": "John Doe", "email": "<EMAIL_ADDRESS>", "age": 30}
"""
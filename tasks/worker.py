"""
Celery Worker Configuration
Background task processing with Redis

Features:
- Task execution
- Task scheduling
- Result storage
- Task monitoring
- Retry logic

Generated by inquantic-foundry
"""

from celery import Celery
from celery.schedules import crontab
from datetime import timedelta

from config.settings import settings
from app_logging.logger import get_logger

logger = get_logger(__name__)

# ============================================================================
# CELERY APPLICATION
# ============================================================================

celery_app = Celery(
    "inquantic_tasks",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND,
    include=["tasks.worker"]  # Auto-discover tasks in this module
)

# ============================================================================
# CELERY CONFIGURATION
# ============================================================================

celery_app.conf.update(
    # Task settings
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,

    # Task execution
    task_acks_late=True,  # Acknowledge task after completion
    task_reject_on_worker_lost=True,
    task_track_started=True,

    # Result backend
    result_expires=3600,  # Results expire after 1 hour
    result_backend_transport_options={
        "master_name": "mymaster",
        "socket_keepalive": True,
    },

    # Worker settings
    worker_prefetch_multiplier=4,
    worker_max_tasks_per_child=1000,  # Restart worker after 1000 tasks
    worker_disable_rate_limits=False,

    # Retry settings
    task_default_retry_delay=60,  # Retry after 60 seconds
    task_max_retries=3,

    # Logging
    worker_redirect_stdouts=True,
    worker_redirect_stdouts_level="INFO",
)

# ============================================================================
# PERIODIC TASKS (Beat Schedule)
# ============================================================================

celery_app.conf.beat_schedule = {
    # Example: Run every day at midnight
    "cleanup-old-data": {
        "task": "tasks.worker.cleanup_old_data",
        "schedule": crontab(hour=0, minute=0),
    },

    # Example: Run every 5 minutes
    "health-check": {
        "task": "tasks.worker.health_check_task",
        "schedule": timedelta(minutes=5),
    },

    # Example: Run every Monday at 9 AM
    "weekly-report": {
        "task": "tasks.worker.generate_weekly_report",
        "schedule": crontab(hour=9, minute=0, day_of_week=1),
    },
}


# ============================================================================
# EXAMPLE TASKS
# ============================================================================

@celery_app.task(
    name="tasks.worker.example_task",
    bind=True,
    max_retries=3,
    default_retry_delay=60
)
def example_task(self, data: dict):
    """
    Example async task.

    Usage:
        from tasks.worker import example_task
        result = example_task.delay({"key": "value"})
        print(result.get())  # Wait for result

    Args:
        data: Task input data

    Returns:
        Task result
    """
    try:
        logger.info("example_task_started", data=data)

        # Your task logic here
        result = {"status": "success", "processed": data}

        logger.info("example_task_completed", result=result)
        return result

    except Exception as exc:
        logger.error("example_task_failed", error=str(exc))
        # Retry on failure
        raise self.retry(exc=exc)


@celery_app.task(name="tasks.worker.send_email")
def send_email_task(to: str, subject: str, body: str):
    """
    Send email asynchronously.

    Usage:
        send_email_task.delay(
            to="user@example.com",
            subject="Welcome",
            body="Welcome to our service!"
        )
    """
    try:
        logger.info("send_email_task_started", to=to, subject=subject)

        # TODO: Implement actual email sending
        # import smtplib
        # ...

        logger.info("send_email_task_completed", to=to)
        return {"status": "sent", "to": to}

    except Exception as e:
        logger.error("send_email_task_failed", error=str(e), to=to)
        raise


@celery_app.task(name="tasks.worker.process_data")
def process_data_task(data_id: str):
    """
    Process data asynchronously.

    Example: Data transformation, ML inference, report generation
    """
    try:
        logger.info("process_data_task_started", data_id=data_id)

        # TODO: Implement data processing logic
        # 1. Fetch data from database
        # 2. Process/transform data
        # 3. Store results

        logger.info("process_data_task_completed", data_id=data_id)
        return {"status": "processed", "data_id": data_id}

    except Exception as e:
        logger.error("process_data_task_failed", error=str(e), data_id=data_id)
        raise


@celery_app.task(name="tasks.worker.cleanup_old_data")
def cleanup_old_data():
    """
    Periodic task to cleanup old data.
    Runs daily at midnight (configured in beat_schedule).
    """
    try:
        logger.info("cleanup_task_started")

        # TODO: Implement cleanup logic
        # 1. Find old records (e.g., > 90 days)
        # 2. Delete or archive them

        cleaned_count = 0  # Placeholder

        logger.info("cleanup_task_completed", cleaned_count=cleaned_count)
        return {"status": "completed", "cleaned_count": cleaned_count}

    except Exception as e:
        logger.error("cleanup_task_failed", error=str(e))
        raise


@celery_app.task(name="tasks.worker.health_check_task")
def health_check_task():
    """
    Periodic health check task.
    Runs every 5 minutes (configured in beat_schedule).
    """
    try:
        logger.info("health_check_task_started")

        # TODO: Implement health checks
        # 1. Check database connectivity
        # 2. Check external service availability
        # 3. Check disk space, memory, etc.

        health_status = {
            "status": "healthy",
            "database": "up",
            "redis": "up",
            "disk_space": "ok"
        }

        logger.info("health_check_task_completed", health=health_status)
        return health_status

    except Exception as e:
        logger.error("health_check_task_failed", error=str(e))
        return {"status": "unhealthy", "error": str(e)}


@celery_app.task(name="tasks.worker.generate_weekly_report")
def generate_weekly_report():
    """
    Generate weekly report.
    Runs every Monday at 9 AM (configured in beat_schedule).
    """
    try:
        logger.info("weekly_report_task_started")

        # TODO: Implement report generation
        # 1. Gather data from last week
        # 2. Generate report
        # 3. Send to stakeholders

        logger.info("weekly_report_task_completed")
        return {"status": "generated", "period": "last_week"}

    except Exception as e:
        logger.error("weekly_report_task_failed", error=str(e))
        raise


# ============================================================================
# TASK GROUPS & CHAINS
# ============================================================================

@celery_app.task(name="tasks.worker.chain_example")
def chain_example():
    """
    Example of chaining tasks (execute in sequence).

    Usage:
        from celery import chain
        workflow = chain(
            process_data_task.s("data_1"),
            send_email_task.s("user@example.com", "Complete", "Processing done")
        )
        workflow.apply_async()
    """
    pass


@celery_app.task(name="tasks.worker.group_example")
def group_example():
    """
    Example of grouping tasks (execute in parallel).

    Usage:
        from celery import group
        job = group(
            process_data_task.s("data_1"),
            process_data_task.s("data_2"),
            process_data_task.s("data_3")
        )
        result = job.apply_async()
    """
    pass


# ============================================================================
# CELERY EVENTS
# ============================================================================

@celery_app.task(bind=True)
def task_with_progress(self, total: int):
    """
    Example task that reports progress.

    Usage:
        result = task_with_progress.delay(100)
        # Check progress: result.info
    """
    for i in range(total):
        # Update task state with progress
        self.update_state(
            state="PROGRESS",
            meta={"current": i, "total": total, "percent": int((i / total) * 100)}
        )

        # Do work here
        import time
        time.sleep(0.1)

    return {"status": "complete", "total": total}


# ============================================================================
# RUNNING THE WORKER
# ============================================================================

"""
To run the Celery worker:

    celery -A tasks.worker worker --loglevel=info

To run the Celery beat scheduler (for periodic tasks):

    celery -A tasks.worker beat --loglevel=info

To run both worker and beat together:

    celery -A tasks.worker worker --beat --loglevel=info

To monitor tasks (Flower):

    pip install flower
    celery -A tasks.worker flower

Then open: http://localhost:5555
"""

# ============================================================================
# EXAMPLE USAGE IN ROUTES
# ============================================================================

"""
# In your router:

from fastapi import APIRouter
from tasks.worker import example_task, send_email_task, process_data_task

router = APIRouter()

@router.post("/tasks/process")
async def trigger_processing(data_id: str):
    # Trigger async task
    task = process_data_task.delay(data_id)

    return {
        "task_id": task.id,
        "status": "processing",
        "message": "Task started in background"
    }

@router.get("/tasks/{task_id}/status")
async def get_task_status(task_id: str):
    # Check task status
    from celery.result import AsyncResult
    task = AsyncResult(task_id, app=celery_app)

    return {
        "task_id": task_id,
        "status": task.state,
        "result": task.result if task.ready() else None
    }
"""